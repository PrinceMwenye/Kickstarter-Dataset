{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d73d62-5afa-43c8-a20c-c5ee09d240c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "import scipy\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd1ef60-fd77-4d8e-afb2-9bc825ca1c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('kickstart.csv', engine='python')\n",
    "\n",
    "#Missing Values\n",
    "\n",
    "dataset.isnull().sum()\n",
    "dataset = dataset[dataset.location.notnull()]\n",
    "dataset.isnull().sum()\n",
    "dataset = dataset[dataset['reward levels'].notnull()] #just remove, hard to determine\n",
    "\n",
    "dataset.pledged =  dataset.pledged.fillna(0) #some had 0 pledges same assumption\n",
    "\n",
    "dataset.isnull().sum() #now we have cleared missing values\n",
    "\n",
    "\n",
    "dataset.status.value_counts()\n",
    "dataset = dataset[dataset.status != 'live']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9681363f-4db5-41c2-a45b-24c7cf0a133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRELIMINARY Analysis\n",
    "\n",
    "duplicated = dataset[dataset.duplicated()] #78 duplicates\n",
    "\n",
    "dataset = dataset.drop_duplicates()\n",
    "\n",
    "dataset.groupby('status')['status'].value_counts()\n",
    "#54% successful\n",
    "#46% successful\n",
    "\n",
    "dataset.goal.mean() #10593 average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be74eef-838a-4e15-9178-76fdfb63a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING\n",
    "fail = ['failed', 'canceled', 'suspended']\n",
    "\n",
    "dataset['status'] = ['failed' if i in fail else 'successful' for i in dataset.status]\n",
    "#LOCATION\n",
    "def cleaner(i):\n",
    "        try:\n",
    "            return str(i).split(',')[1].lstrip() \n",
    "        except:\n",
    "            str(i).split(',')[0].lstrip() \n",
    "            \n",
    "dataset['location'] = dataset.location.map(cleaner)\n",
    "\n",
    "#DATE\n",
    "datemaker = lambda i: datetime.datetime.strptime((' '.join(i.split()[:5])).replace(',', ''), '%a %d  %b %Y %H:%M:%S')\n",
    "#sparse date into datetime\n",
    "\n",
    "dataset['funded date'] = dataset['funded date'].map(datemaker) #get full funded date\n",
    "\n",
    "dataset['start date'] = [i - datetime.timedelta(j) for i, j in zip(dataset['funded date'], dataset['duration'])] #full start date of campaign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856d395-bb87-4efc-b64c-0306131d7273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#date and month\n",
    "\n",
    "dataset['full_date'] = [str(i).split()[0] for i in dataset['start date']]\n",
    "\n",
    "def datemaker_date(d):\n",
    "    year,month,date = [int(i) for i in d.split('-')]\n",
    "    launch = datetime.date(year,month,date)\n",
    "    return launch.strftime('%a')\n",
    "\n",
    "dataset['day_of_launch'] = dataset['full_date'].map(datemaker_date)\n",
    "\n",
    "def datemaker_month(d):\n",
    "    year,month,date = [int(i) for i in d.split('-')]\n",
    "    launch = datetime.date(year,month,date)\n",
    "    return launch.strftime('%b')\n",
    "\n",
    "dataset['month_of_launch'] = dataset['full_date'].map(datemaker_month)\n",
    "\n",
    "\n",
    "def datemaker_time(t):\n",
    "    hour,minute,second = [int(i) for i in t.split(':')]\n",
    "    time = datetime.time(hour,minute,second)\n",
    "    return time.strftime('%p')\n",
    "\n",
    "\n",
    "dataset['time_of_launch'] = [str(i).split()[1] for i in dataset['start date']]\n",
    "\n",
    "\n",
    "dataset['AM/PM'] = dataset.time_of_launch.map(datemaker_time).astype('category').map(str)\n",
    "\n",
    "\n",
    "\n",
    "dataset.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa55bb-c244-4d08-9e37-220d4b61db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAME\n",
    "dataset['length_of_name'] = [len(i.split()) for i in dataset.name] #length of name important?\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "def special_name(n):\n",
    "    special = ['[','$','#','%','@','=','+','/','&','Ɛ','™','©','?','!','¡','.','*',']','»','«','=']\n",
    "    if n.isupper():\n",
    "        return 'yes'\n",
    "    if any(i in n for i in special):\n",
    "        return 'yes'\n",
    "    if any(i.isupper() for i in n.split()):\n",
    "        return 'yes'\n",
    "    else:\n",
    "        return 'no'\n",
    "    \n",
    "dataset['special_name'] = dataset['name'].map(special_name)\n",
    "#special names useful to grab attention according google ads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a25cd6-b4bc-41a3-b73a-90011c357663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset= dataset.drop(['funded date', 'start date', 'full_date', 'name', 'project id', 'url', 'reward levels', 'subcategory' ], axis = 1)\n",
    "\n",
    "dataset['pledged'].mean() \n",
    "\n",
    "dataset.iloc[:, [0,1,-1,-3,-5,-6]] = dataset.iloc[:, [0,1,-1,-3,-5, -6]].astype('category')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(dataset['backers'], bins = 50)\n",
    "print(skew(dataset['backers'].values, axis = 0, bias = False))\n",
    "#right skew of backers\n",
    "#skewness of 87 means highly skewed\n",
    "scipy.stats.normaltest(dataset['backers'])\n",
    "#p value < than level of significance, hence not even normally distributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e4733-4eb1-4ea9-b4ac-6dbcf28f4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(skew(dataset['duration'], bias = False))\n",
    "#skew > 1 hence not normal\n",
    "scipy.stats.normaltest(dataset['duration'])\n",
    "#p value < than level of significance, hence not even normally distributed\n",
    "#reject null hypothesis of normal\n",
    "\n",
    "dataset['status'].value_counts()\n",
    "\n",
    "\n",
    "#Hypothesis\n",
    "\n",
    "#status by day_of_launch\n",
    "day_success = pd.DataFrame(dataset.groupby('day_of_launch')['status'].value_counts())\n",
    "day_success.groupby('day_of_launch').apply(lambda i: 100 * i/i.sum())\n",
    "day_success['percentage'] = (day_success['successful']/(day_success['failed'] + day_success['successful']))\n",
    "day_success.percentage.sort_values()\n",
    "#Tues, Monday, Wed\n",
    " \n",
    "\n",
    "\n",
    "month_success  = pd.DataFrame(dataset.groupby('month_of_launch')['status'].value_counts().unstack()) #first quatter of month\n",
    "month_success['percentage'] = month_success['successful']/(month_success['failed'] + month_success['successful'])\n",
    "mdf = pd.DataFrame(month_success.percentage.sort_values(ascending = False))\n",
    "#dec, jan feb area + April\n",
    "\n",
    "#success by duration\n",
    "\n",
    "dataset.groupby('status')['duration'].mean() #43 for failed 38 for succesful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1489d-e7ec-46f1-87a5-e8ec8db99b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#success by goal\n",
    "dataset.groupby('status')['goal'].mean()\n",
    "# $16762 failed, $5524 successful\n",
    "#lower the goal, the more likely it will be succesful\n",
    "\n",
    "\n",
    "#success by category\n",
    "\n",
    "category_success = dataset.groupby('category')['status'].value_counts().unstack()\n",
    "category_success['percentage'] = category_success['successful']/(category_success['failed'] + category_success['successful']) \n",
    "category_success.percentage.sort_values()\n",
    "#Dance, Theatre, Music, \n",
    "#least are fashion, tech\n",
    "\n",
    "#success by updates\n",
    "dataset.groupby('status')['updates'].mean()\n",
    "#success - 7\n",
    "#fail - 1\n",
    "# need more updates to potential donors, engagement\n",
    "\n",
    "#success by length of name\n",
    "\n",
    "namecounter = lambda i: len(i.split())\n",
    "dataset.groupby('status')['length_of_name'].mean()\n",
    "#not much diff, 6 and 6\n",
    "\n",
    "#success by location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da347421-043f-400b-8d4f-68fc17d763dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset.location.notnull()]\n",
    "\n",
    "dataset.location = ['USA' if len(i) == 2 else i for i in dataset.location] #for smaller numpy object\n",
    "location_success = pd.DataFrame(dataset.groupby('location')['status'].value_counts().unstack())\n",
    "#top locations are CA, NY, IL, TX, MA, WA, \n",
    "\n",
    "#but most succesful based on launch then success are Jordan, Indonesia, Singapore, Iceland, Tunisia\n",
    "#US states RI, VT, MT\n",
    "\n",
    "#ANALYSIS \n",
    "\n",
    "\n",
    "x = dataset.iloc[:, [0, 3,9, 8, 10, 11,12,14,15,16 ] ].values\n",
    "y = dataset.iloc[:, 2].values\n",
    "\n",
    "k = dataset.iloc[:, [0, 3,9, 8, 10, 11,12,14,15,16 ] ]\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(sparse= False), [0,-1,-3,-5,-4])], remainder='passthrough')\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "\n",
    "x = np.array(ct.fit_transform(x))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_test, x_train, y_test, y_train = train_test_split(x,y, test_size = 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221566b-a3bb-4ff3-accb-eee55d0cb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE SCALING\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train[:, 37:] = sc.fit_transform(x_train[:, 37:])\n",
    "x_test[:, 37:] = sc.transform(x_test[:, 37:])\n",
    "\n",
    "#models RANDOM FOREST, XGBOOST, BAYES\n",
    "\n",
    "#RANDOM FOREST\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "Random_classifier = RandomForestClassifier(n_estimators = 500, \n",
    "                                           criterion = 'entropy',\n",
    "                                           max_depth = 25,\n",
    "                                           min_samples_split = 3,\n",
    "                                           random_state = 0)\n",
    "Random_classifier.fit(x_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = Random_classifier.predict(x_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "Random_accuracy = accuracy_score(y_test, y_pred)\n",
    "#84 % accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a254b5d-49ac-4eb3-954e-f2f51d2c1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#NAIVE BAYES\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive_classifier = GaussianNB()\n",
    "naive_classifier.fit(x_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred2 = naive_classifier.predict(x_test)\n",
    "naive_accuracy = accuracy_score(y_test, y_pred2)\n",
    "#K - NEAREST\n",
    "# Training the K-NN model on the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k_nearest_classifier = KNeighborsClassifier(n_neighbors = 150, metric = 'minkowski', p = 2)\n",
    "k_nearest_classifier.fit(x_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred3 = k_nearest_classifier.predict(x_test)\n",
    "k_nearest_accuracy = accuracy_score(y_test, y_pred3)\n",
    "#XG-BOOST\n",
    "# Fitting XGBoost to the Training set\n",
    "from xgboost import XGBClassifier\n",
    "xg_classifier = XGBClassifier()\n",
    "xg_classifier.fit(x_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred4 = xg_classifier.predict(x_test)\n",
    "xg_accuracy = accuracy_score(y_test, y_pred4)\n",
    "#GRID SEARCH\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'n_estimators': [100, 500],\n",
    "               'max_depth': [10,20, 25],\n",
    "               'min_samples_split': [1.5,3]}]\n",
    "   \n",
    "gs = GridSearchCV(estimator = Random_classifier,\n",
    "                  param_grid = parameters,\n",
    "                  scoring = 'accuracy',\n",
    "                  cv = 6,\n",
    "                  n_jobs = -1)\n",
    "gs = gs.fit(x_train, y_train)\n",
    "best_accuracy = gs.best_score_\n",
    "best_params = gs.best_params_     \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c43de-fab5-4b7d-ae9c-9e3d9127d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = Random_classifier, X = x_train, y = y_train, cv = 10)\n",
    "accuracies.mean()\n",
    "accuracies.std()\n",
    "\n",
    "#after the best accuracy lets get feature importances\n",
    "feature_importances = Random_classifier.feature_importances_\n",
    "cat_one_hot_att = ['category', 'special_name', 'AM/PM', 'day_of_launch', 'month_of_launch']\n",
    "num_attributes = ['goal', 'comments', 'updates', 'duration', 'length_of_name']\n",
    "attributes = num_attributes + cat_one_hot_att\n",
    "vals = sorted(zip(feature_importances, attributes), key=lambda x: x[0], reverse=True)\n",
    "df = pd.DataFrame(vals)\n",
    "df.iloc[:, -1] = df.iloc[:, -1].replace({i : k for i, k in enumerate(cat_one_hot_att)})\n",
    "\n",
    "#splits help determine classes, most effectively\n",
    "    \n",
    "plt.scatter(df[0], df[1])\n",
    "trial =  dataset[dataset['status']== 'successful']\n",
    "plt.scatter(trial['time_of_launch'], trial['status'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
